{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64719032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import get_linear_schedule_with_warmup  # scheduler\n",
    "from torch.optim import AdamW                           # optimizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5def3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea1f2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl_flat(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        records = [json.loads(line) for line in f if line.strip()]\n",
    "    return json_normalize(records)\n",
    "\n",
    "train_data = load_jsonl_flat('../data/raw/train.jsonl')\n",
    "kaggle_data = load_jsonl_flat('../data/raw/kaggle_test.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd735a3",
   "metadata": {},
   "source": [
    "# Extract tweet text + metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae99086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Extract full text\n",
    "# ----------------------------------------------------\n",
    "def extract_full_text(tweet):\n",
    "    text = tweet.get('text','')\n",
    "    extended = tweet.get('extended_tweet.full_text')\n",
    "    if extended and not pd.isna(extended):\n",
    "        text = extended\n",
    "    return text\n",
    "\n",
    "train_data['full_text'] = train_data.apply(lambda row: extract_full_text(row.to_dict()), axis=1)\n",
    "kaggle_data['full_text'] = kaggle_data.apply(lambda row: extract_full_text(row.to_dict()), axis=1)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Save target column before dropping\n",
    "# ----------------------------------------------------\n",
    "y_train = train_data['label'].copy()\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Keep train columns aligned with test set\n",
    "# ----------------------------------------------------\n",
    "train_data = train_data.dropna(how='all', axis=\"columns\")\n",
    "train_data = train_data.drop(\n",
    "    train_data.columns.difference(kaggle_data.columns).to_list(), axis=1\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Identify metadata columns\n",
    "# ----------------------------------------------------\n",
    "text_column = 'full_text'\n",
    "categorical_column = 'source'\n",
    "\n",
    "num_columns = train_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "bool_columns = train_data.select_dtypes(include=[np.bool_]).columns.tolist()\n",
    "list_columns = [col for col in train_data.columns if train_data[col].apply(lambda x: isinstance(x,list)).any()]\n",
    "\n",
    "unuseful_columns = [\n",
    "    \"lang\", \"text\", \"extended_tweet.full_text\", \"user.description\",\n",
    "    'retweet_count', 'favorite_count', 'quote_count', 'reply_count',\n",
    "    'retweeted', 'favorited', 'user.default_profile_image',\n",
    "    'user.protected', 'user.contributors_enabled'\n",
    "]\n",
    "\n",
    "num_columns = [col for col in num_columns if col not in unuseful_columns]\n",
    "bool_columns = [col for col in bool_columns if col not in unuseful_columns]\n",
    "list_columns = [col for col in list_columns if col not in unuseful_columns]\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Function to extract metadata features\n",
    "# ----------------------------------------------------\n",
    "def extract_features(df, num_columns, bool_columns, list_columns, unuseful_columns):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Numerical columns: fill NAs and replace inf\n",
    "    df[num_columns] = df[num_columns].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    # Boolean columns: convert True/False -> 1/0\n",
    "    for column in bool_columns:\n",
    "        df[column] = df[column].map({True: 1, False: 0})\n",
    "    \n",
    "    # List columns: encode as length of the list\n",
    "    for col in list_columns:\n",
    "        df[col] = df[col].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "    \n",
    "    # Drop unuseful columns\n",
    "    df = df.drop(unuseful_columns, axis=1, errors='ignore')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Apply feature extraction\n",
    "# ----------------------------------------------------\n",
    "X_train_f = extract_features(train_data, num_columns, bool_columns, list_columns, unuseful_columns)\n",
    "X_test_f = extract_features(kaggle_data, num_columns, bool_columns, list_columns, unuseful_columns)\n",
    "\n",
    "# Keep only numeric columns shared between train/test\n",
    "common_numeric_cols = X_train_f.select_dtypes(include=[np.number]).columns.intersection(\n",
    "    X_test_f.select_dtypes(include=[np.number]).columns\n",
    ")\n",
    "\n",
    "metadata_train = X_train_f[common_numeric_cols].values.astype(np.float32)\n",
    "metadata_test = X_test_f[common_numeric_cols].values.astype(np.float32)\n",
    "\n",
    "# Standardize metadata\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "metadata_train = scaler.fit_transform(metadata_train)\n",
    "metadata_test = scaler.transform(metadata_test)\n",
    "\n",
    "print(\"Metadata shape (train):\", metadata_train.shape)\n",
    "print(\"Metadata shape (test):\", metadata_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c7240d",
   "metadata": {},
   "source": [
    "# Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc160fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"camembert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "bert_model = AutoModel.from_pretrained(MODEL_NAME).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d74939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, metadata, labels=None, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.metadata = torch.tensor(metadata, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long) if labels is not None else None\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = tokens['input_ids'].squeeze(0)\n",
    "        attention_mask = tokens['attention_mask'].squeeze(0)\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'metadata': self.metadata[idx]\n",
    "        }\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = self.labels[idx]\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643327a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 1. Create Train/Validation Split\n",
    "# ----------------------------------------------------\n",
    "train_texts, val_texts, train_meta, val_meta, train_labels, val_labels = train_test_split(\n",
    "    train_data['full_text'].tolist(),\n",
    "    metadata_train,\n",
    "    y_train.values,\n",
    "    test_size=0.1,       # 20% for validation\n",
    "    random_state=42,\n",
    "    stratify=y_train.values\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_texts)}\")\n",
    "print(f\"Val size: {len(val_texts)}\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2. Instantiate Datasets\n",
    "# ----------------------------------------------------\n",
    "train_dataset = TweetDataset(train_texts, train_meta, train_labels)\n",
    "val_dataset = TweetDataset(val_texts, val_meta, val_labels)\n",
    "\n",
    "# Create the test dataset (Kaggle)\n",
    "kaggle_dataset = TweetDataset(\n",
    "    kaggle_data['full_text'].tolist(), \n",
    "    metadata_test, \n",
    "    labels=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f8130f",
   "metadata": {},
   "source": [
    "# CamemBERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f9588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CamemBERTClassifier(nn.Module):\n",
    "    def __init__(self, bert_model, metadata_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(bert_model.config.hidden_size + metadata_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, metadata):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_emb = outputs.last_hidden_state[:,0,:]  # CLS token\n",
    "        x = torch.cat([cls_emb, metadata], dim=1)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.classifier(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3f08bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def freeze_bert_layers(model, num_unfrozen_last_layers=3):\n",
    "    total_layers = model.bert.config.num_hidden_layers\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        \n",
    "        # We only care about freezing the encoder layers\n",
    "        if \"encoder.layer\" in name:\n",
    "            # Use Regex to find the pattern \"layer.<number>\"\n",
    "            match = re.search(r\"encoder\\.layer\\.(\\d+)\", name)\n",
    "            \n",
    "            if match:\n",
    "                layer_idx = int(match.group(1))\n",
    "                \n",
    "                if layer_idx < total_layers - num_unfrozen_last_layers:\n",
    "                    param.requires_grad = False\n",
    "                else:\n",
    "                    param.requires_grad = True\n",
    "        \n",
    "        # Always keep the classifier head and embeddings trainable\n",
    "        else:\n",
    "            param.requires_grad = True\n",
    "\n",
    "    print(f\"Freezing complete. Last {num_unfrozen_last_layers} layers are unfrozen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb219d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, val_dataset, device, epochs=3, batch_size=16, lr=2e-5):\n",
    "    # Create DataLoaders from the Dataset objects\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    \n",
    "    # Warmup for 6% of total steps\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=int(0.06 * total_steps), \n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.to(device)\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # --- Training Phase ---\n",
    "        model.train()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for batch in tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            metadata = batch['metadata'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask, metadata=metadata)\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        train_acc = correct / total\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                metadata = batch['metadata'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                logits = model(input_ids=input_ids, attention_mask=attention_mask, metadata=metadata)\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        \n",
    "        val_acc = val_correct / val_total\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), \"best_camembert_meta.pt\")\n",
    "            \n",
    "    print(\"Training complete. Best val acc:\", best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c57ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Initialize and train model\n",
    "# --------------------------\n",
    "meta_dim = metadata_train.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "model = CamemBERTClassifier(bert_model, metadata_dim=meta_dim, num_classes=num_classes)\n",
    "\n",
    "# Freeze layers\n",
    "freeze_bert_layers(model, num_unfrozen_last_layers=7)\n",
    "\n",
    "train_model(\n",
    "    model, \n",
    "    train_dataset=train_dataset, \n",
    "    val_dataset=val_dataset, \n",
    "    device=device, \n",
    "    epochs=3, \n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b5d8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataset, device, batch_size=32):\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            metadata = batch['metadata'].to(device)\n",
    "            \n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask, metadata=metadata)\n",
    "            batch_preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            preds.extend(batch_preds.tolist())\n",
    "            \n",
    "    return np.array(preds)\n",
    "\n",
    "model.load_state_dict(torch.load(\"best_camembert_meta.pt\", map_location=device))\n",
    "kaggle_preds = predict(model, kaggle_dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de3ccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Save submission\n",
    "# --------------------------\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": X_test_f[\"challenge_id\"].astype(int),\n",
    "    \"Prediction\": kaggle_preds\n",
    "})\n",
    "submission.to_csv(\"camembert_with_meta_submission.csv\", index=False)\n",
    "print(\"Saved camembert_with_meta_submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
