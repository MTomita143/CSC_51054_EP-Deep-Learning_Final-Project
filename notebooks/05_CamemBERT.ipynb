{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b9f5d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/milapopovic/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Data handling libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "\n",
    "# Natural Language Processing (NLP) libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Scikit-learn modeling libraries\n",
    "from sklearn.dummy import DummyClassifier # For baseline model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # To convert text to numbers\n",
    "from sklearn.linear_model import LogisticRegression # The classifier model\n",
    "from sklearn.metrics import accuracy_score, classification_report # For evaluation\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score # For splitting and validating\n",
    "from sklearn.pipeline import Pipeline # To chain processing steps\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcfc2d1",
   "metadata": {},
   "source": [
    "# 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ce78a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl_flat(path):\n",
    "    \"\"\"Load a JSON Lines file and flatten nested structures.\"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        records = [json.loads(line) for line in f if line.strip()]\n",
    "    return json_normalize(records)\n",
    "\n",
    "# --- Load and flatten training data ---\n",
    "train_data = load_jsonl_flat('../data/raw/train.jsonl')\n",
    "\n",
    "# --- Load and flatten Kaggle test data ---\n",
    "kaggle_data = load_jsonl_flat('../data/raw/kaggle_test.jsonl')\n",
    "\n",
    "# --- Separate features and target for training ---\n",
    "X_train = train_data.drop(columns=['label'])\n",
    "y_train = train_data['label']\n",
    "\n",
    "# --- Features for Kaggle test set ---\n",
    "X_kaggle = kaggle_data  # Kaggle test set usually has no label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602f5807",
   "metadata": {},
   "source": [
    "# 2. Transform to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f31fbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get the full text from a tweet object.\n",
    "# Tweets can be truncated, storing the full version in 'extended_tweet.full_text'.\n",
    "def extract_full_text(tweet):\n",
    "    # Start with the standard 'text' field\n",
    "    text = tweet['text']\n",
    "    # Check if the 'extended_tweet.full_text' field exists (is not NaN)\n",
    "    if not pd.isna(tweet['extended_tweet.full_text']):\n",
    "        # If it exists, it's the full text, so use it instead\n",
    "        text = tweet['extended_tweet.full_text']\n",
    "    return text\n",
    "\n",
    "# Apply this function to every row (axis=1) in the training data\n",
    "X_train['full_text'] = X_train.apply(lambda tweet: extract_full_text(tweet), axis=1)\n",
    "# Apply the same function to the Kaggle test data\n",
    "X_kaggle['full_text'] = X_kaggle.apply(lambda tweet: extract_full_text(tweet), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2746ce63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/milapopovic/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "# Load a list of common French stop words (e.g., 'le', 'la', 'de')\n",
    "french_stop_words = stopwords.words('french')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9075042f",
   "metadata": {},
   "source": [
    "# 3. CamemBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20cab75",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "MODEL_NAME = \"camembert-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "bert_model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "bert_model.eval()\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# CAMEMBERT EMBEDDINGS\n",
    "# ----------------------------------------------------\n",
    "def embed_text_camembert(texts, batch_size=16):\n",
    "    \"\"\"Return a numpy matrix of CamemBERT embeddings (CLS token).\"\"\"\n",
    "    all_embeddings = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "\n",
    "        tokens = tokenizer(\n",
    "            batch,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(**tokens)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]  \n",
    "\n",
    "        all_embeddings.append(cls_embeddings.cpu().numpy())\n",
    "\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# METADATA EXTRACTION\n",
    "# ----------------------------------------------------\n",
    "def extract_features(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    df['tweet_length'] = df['full_text'].apply(lambda x: len(str(x)))\n",
    "    df['word_count'] = df['full_text'].apply(lambda x: len(str(x).split()))\n",
    "    df['hashtags_count'] = df['full_text'].apply(lambda x: str(x).count('#'))\n",
    "    df['mentions_count'] = df['full_text'].apply(lambda x: str(x).count('@'))\n",
    "    df['urls_count'] = df['full_text'].apply(lambda x: str(x).count('http'))\n",
    "\n",
    "    # â˜… Your most predictive non-text feature\n",
    "    df['total_tweets'] = df['user.statuses_count']\n",
    "\n",
    "    # Simple binary feature\n",
    "    df['is_location_available'] = df['user.location'].apply(\n",
    "        lambda x: 0 if pd.isna(x) or x == '' else 1\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# APPLY FEATURE ENGINEERING\n",
    "# ----------------------------------------------------\n",
    "X_train_f = extract_features(X_train)\n",
    "X_test_f = extract_features(X_kaggle)\n",
    "\n",
    "numeric_columns = [\n",
    "    'tweet_length', 'word_count', 'hashtags_count', 'mentions_count', 'urls_count',\n",
    "    'total_tweets', 'is_location_available'\n",
    "]\n",
    "\n",
    "metadata_train = X_train_f[numeric_columns].values\n",
    "metadata_test = X_test_f[numeric_columns].values\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# EMBED TEXT\n",
    "# ----------------------------------------------------\n",
    "print(\"Embedding training tweets with CamemBERT...\")\n",
    "bert_train = embed_text_camembert(X_train_f[\"full_text\"].tolist())\n",
    "\n",
    "print(\"Embedding test tweets with CamemBERT...\")\n",
    "bert_test = embed_text_camembert(X_test_f[\"full_text\"].tolist())\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# COMBINE FEATURES\n",
    "# ----------------------------------------------------\n",
    "X_train_combined = np.hstack([bert_train, metadata_train])\n",
    "X_test_combined = np.hstack([bert_test, metadata_test])\n",
    "\n",
    "print(\"Final feature shape:\", X_train_combined.shape)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# TRAIN XGBOOST\n",
    "# ----------------------------------------------------\n",
    "clf = XGBClassifier(\n",
    "    n_estimators=300,        # reduced to avoid kernel crash\n",
    "    max_depth=5,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric=\"logloss\"\n",
    ")\n",
    "\n",
    "clf.fit(X_train_combined, y_train)\n",
    "\n",
    "y_pred_test = clf.predict(X_test_combined)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# SAVE PREDICTIONS\n",
    "# ----------------------------------------------------\n",
    "output = pd.DataFrame({\n",
    "    \"ID\": X_test_f[\"challenge_id\"].astype(int),\n",
    "    \"Prediction\": y_pred_test\n",
    "})\n",
    "\n",
    "output.to_csv(\"camembert_metadata_submission.csv\", index=False)\n",
    "print(\"Saved camembert_metadata_submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
